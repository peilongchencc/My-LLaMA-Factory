# ä»‹ç»ä»¥dockeræ–¹å¼å¯åŠ¨llamafactory(é€‚ç”¨äºCUDAç”¨æˆ·)

æœ¬ç« ä»‹ç»å¦‚ä½•ä»¥dockeræ–¹å¼å¯åŠ¨llamafactory(é€‚ç”¨äºCUDAç”¨æˆ·)ã€‚

- [ä»‹ç»ä»¥dockeræ–¹å¼å¯åŠ¨llamafactory(é€‚ç”¨äºCUDAç”¨æˆ·)](#ä»‹ç»ä»¥dockeræ–¹å¼å¯åŠ¨llamafactoryé€‚ç”¨äºcudaç”¨æˆ·)
  - [å‰è¨€(å¯é€‰):](#å‰è¨€å¯é€‰)
    - [å¯åŠ¨ Docker æœåŠ¡:](#å¯åŠ¨-docker-æœåŠ¡)
    - [å°† Docker æ·»åŠ åˆ°å¯åŠ¨é¡¹ï¼Œä»¥ç¡®ä¿åœ¨ç³»ç»Ÿé‡æ–°å¯åŠ¨æ—¶ Docker ä¼šè‡ªåŠ¨å¯åŠ¨ï¼š](#å°†-docker-æ·»åŠ åˆ°å¯åŠ¨é¡¹ä»¥ç¡®ä¿åœ¨ç³»ç»Ÿé‡æ–°å¯åŠ¨æ—¶-docker-ä¼šè‡ªåŠ¨å¯åŠ¨)
  - [å…‹éš†LLaMA-Factoryä»“åº“:](#å…‹éš†llama-factoryä»“åº“)
  - [åˆ‡æ¢åˆ°docker-cudaç›®å½•](#åˆ‡æ¢åˆ°docker-cudaç›®å½•)
  - [æ‹‰å–é•œåƒå¹¶å¯åŠ¨å®¹å™¨:](#æ‹‰å–é•œåƒå¹¶å¯åŠ¨å®¹å™¨)
  - [æŸ¥çœ‹å®¹å™¨ä¿¡æ¯(å¯é€‰):](#æŸ¥çœ‹å®¹å™¨ä¿¡æ¯å¯é€‰)
  - [è¿›å…¥llamafactoryå®¹å™¨æ“ä½œ:](#è¿›å…¥llamafactoryå®¹å™¨æ“ä½œ)
  - [é…ç½®ä½¿ç”¨é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹(å¯é€‰):](#é…ç½®ä½¿ç”¨é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹å¯é€‰)
  - [å¯åŠ¨ LLaMA Board å¯è§†åŒ–å¾®è°ƒï¼ˆç”± Gradio é©±åŠ¨ï¼‰:](#å¯åŠ¨-llama-board-å¯è§†åŒ–å¾®è°ƒç”±-gradio-é©±åŠ¨)
  - [æ•°æ®å·è¯¦æƒ…(å¯é€‰):](#æ•°æ®å·è¯¦æƒ…å¯é€‰)
  - [æ— æ³•ä»¥Dockeræ–¹å¼å¯åŠ¨llamafactoryè§£å†³æ–¹æ¡ˆ(å¯é€‰):](#æ— æ³•ä»¥dockeræ–¹å¼å¯åŠ¨llamafactoryè§£å†³æ–¹æ¡ˆå¯é€‰)
    - [æ–¹æ³•ä¸€: nvidiaå®˜æ–¹æä¾›çš„å®‰è£…æ–¹å¼:](#æ–¹æ³•ä¸€-nvidiaå®˜æ–¹æä¾›çš„å®‰è£…æ–¹å¼)
      - [1. é…ç½®ä»£ç ä»“åº“:](#1-é…ç½®ä»£ç ä»“åº“)
      - [2. æ›´æ–°åŒ…åˆ—è¡¨:](#2-æ›´æ–°åŒ…åˆ—è¡¨)
      - [3. å®‰è£…NVIDIA Container Toolkit:](#3-å®‰è£…nvidia-container-toolkit)
    - [æ–¹æ³•äºŒ: æ‰‹åŠ¨å®‰è£…NVIDIA Container Toolkit:](#æ–¹æ³•äºŒ-æ‰‹åŠ¨å®‰è£…nvidia-container-toolkit)
      - [1. ä¸‹è½½å®‰è£…åŒ…:](#1-ä¸‹è½½å®‰è£…åŒ…)
      - [2. å°†å®‰è£…åŒ…ä¸Šä¼ åˆ°æœåŠ¡å™¨:](#2-å°†å®‰è£…åŒ…ä¸Šä¼ åˆ°æœåŠ¡å™¨)
      - [3. è§£å‹æ–‡ä»¶:](#3-è§£å‹æ–‡ä»¶)
      - [4. æŒ‰ä¾èµ–é¡ºåºå®‰è£… .deb æ–‡ä»¶:](#4-æŒ‰ä¾èµ–é¡ºåºå®‰è£…-deb-æ–‡ä»¶)
      - [5. éªŒè¯å®‰è£…:](#5-éªŒè¯å®‰è£…)


## å‰è¨€(å¯é€‰):

ä½¿ç”¨Dockeréƒ¨ç½²LLaMA-Factoryå‰å…ˆç¡®å®šä½ çš„ç³»ç»Ÿæœ‰Dockerï¼Œå¦‚æœè¿˜æ²¡æœ‰å®‰è£…Dockerï¼Œå¯ä»¥æŸ¥çœ‹ç¬”è€…çš„[Docker å®‰è£…æ•™ç¨‹](https://github.com/peilongchencc/docker_tutorial/tree/main/docker_install)ã€‚

Dockerå®‰è£…å¥½åï¼Œè®°å¾—å…ˆå¯åŠ¨ä½ çš„Dockerå†æ‰§è¡Œåç»­æ“ä½œï¼Œä¸è¦Dockeréƒ½æ²¡æœ‰å¯åŠ¨ï¼Œè¿˜é—®ç¬”è€…ä¸ºå•¥Dockeréƒ¨ç½²å¤±è´¥â€¼ï¸â€¼ï¸

### å¯åŠ¨ Docker æœåŠ¡:

å¦‚æœä½ çš„DockeræœåŠ¡æ²¡æœ‰å¯åŠ¨ï¼Œå¯ä»¥è¿è¡Œä»¥ä¸‹æŒ‡ä»¤å¯åŠ¨DockeræœåŠ¡:<br>

```bash
sudo systemctl start docker
```

### å°† Docker æ·»åŠ åˆ°å¯åŠ¨é¡¹ï¼Œä»¥ç¡®ä¿åœ¨ç³»ç»Ÿé‡æ–°å¯åŠ¨æ—¶ Docker ä¼šè‡ªåŠ¨å¯åŠ¨ï¼š

```bash
sudo systemctl enable docker
```

ç»ˆç«¯æ˜¾ç¤º:<br>

```log
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker
```


## å…‹éš†LLaMA-Factoryä»“åº“:

```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
```


## åˆ‡æ¢åˆ°docker-cudaç›®å½•

```bash
cd LLaMA-Factory/docker/docker-cuda/
```


## æ‹‰å–é•œåƒå¹¶å¯åŠ¨å®¹å™¨:

ç»ˆç«¯è¿è¡Œä¸‹åˆ—æŒ‡ä»¤ï¼Œæ‹‰å–ï¼ˆå¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼‰é•œåƒå¹¶å¯åŠ¨å®¹å™¨:

> åç»­å¯åŠ¨å¦‚æœä½ çš„é•œåƒå·²ç»å­˜åœ¨ä¸”æœ€æ–°ï¼Œåˆ™ä¼šè·³è¿‡æ‹‰å–æ­¥éª¤ï¼Œä¼šç›´æ¥å¯åŠ¨å®¹å™¨ã€‚

```bash
docker compose up -d
```

> [!CAUTION]
> docker composeæŒ‡ä»¤éœ€è¦åœ¨æœ‰ `docker-compose.yml` å­˜åœ¨çš„ç›®å½•æ‰§è¡Œã€‚

ğŸš¨æ³¨æ„: LLaMA-Factoryä¸­é€‚ç”¨CUDAç”¨æˆ·çš„pytorchï¼Œé•œåƒæºæ¥è‡ªNGCï¼Œå‹ç¼©åŒ…å¤§å°ä¸º 4.84 GBï¼Œå«æœ‰ä¸‹åˆ—å†…å®¹:

- CUDA
- cuBLAS
- NVIDIA cuDNN
- NVIDIA NCCL (optimized for NVLink)
- RAPIDS
- NVIDIA Data Loading Library (DALI)
- TensorRT
- Torch-TensorRT

ç”±äºå«æœ‰çš„å†…å®¹å¾ˆå¤§ï¼Œæ‰€ä»¥æ‹‰å–é€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œéœ€è¦å¤šç­‰å¾…ä¸€ä¼šã€‚

â€¼ï¸å¦‚æœå¡åœ¨NGCçš„pytorchæ‹‰å–å¤±è´¥ï¼Œå¯ä»¥æ ¹æ®NGCçš„ä¿¡æ¯ä»Docker Hubæˆ–å…¶ä»–èµ„æºæ‹‰å–ã€‚NGCçš„pytorché“¾æ¥:

```log
https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch
```

â€¼ï¸å¦‚æœå¡åœ¨ `pip install` çš„ä½ç½®æŠ¥é”™äº†ï¼Œå¯ä»¥å°†`docker-compose.yml`ä¸­çš„`PIP_INDEX`ä¿®æ”¹ä¸ºå›½å†…pypiæºã€‚ä¾‹å¦‚:

> åˆ«æ‹…å¿ƒpytorchä¼šé‡æ–°ä¸‹è½½ï¼Œpytorché•œåƒå¦‚æœæ‹‰å–æˆåŠŸäº†ï¼Œé‡æ–°è¿è¡Œ`docker compose up -d`ä¼šç›´æ¥ä»pipçš„åœ°æ–¹å¼€å§‹è¿è¡Œã€‚

```bash
services:
  llamafactory:
    build:
      dockerfile: ./docker/docker-cuda/Dockerfile
      context: ../..
      args:
        INSTALL_BNB: false
        INSTALL_VLLM: false
        INSTALL_DEEPSPEED: false
        INSTALL_FLASHATTN: false
        PIP_INDEX: https://mirrors.aliyun.com/pypi/simple
```

> ä¿®æ”¹ `docker-compose.yml` ä¸­çš„ `args` ä¼šè¦†ç›–æˆ–æ‰©å…… `Dockerfile` ä¸­çš„ `ARG` å‚æ•°ã€‚

> [!TIP]
> ç»ˆææ–¹æ¡ˆ: å¦‚æœå„ç§æ–¹å¼å°è¯•äº†éƒ½æ— æ³•æ‹‰å–ï¼Œå°±æ‰¾ä¸ªèƒ½æ‹‰å–ä¸‹æ¥é•œåƒçš„äººï¼Œè®©ä»–æŠŠé•œåƒå‹ç¼©ä¸ºtarå‘ç»™ä½ ğŸ˜

ç»ˆç«¯ç¤ºä¾‹:

```log
(base) root@ubuntu22:~/data/LLaMA-Factory/docker/docker-cuda# docker compose up -d
[+] Building 788.3s (12/12) FINISHED                                                                                                                                        docker:default
 => [llamafactory internal] load build definition from Dockerfile                                                                                                                     0.1s
 => => transferring dockerfile: 1.83kB                                                                                                                                                0.0s
 => [llamafactory internal] load metadata for nvcr.io/nvidia/pytorch:24.02-py3                                                                                                        3.6s
 => [llamafactory internal] load .dockerignore                                                                                                                                        0.0s
 => => transferring context: 147B                                                                                                                                                     0.0s
 => [llamafactory 1/7] FROM nvcr.io/nvidia/pytorch:24.02-py3@sha256:69c54ea51853c57b1f5abae7878a64b238fb10c177855e1c6521d7ab87fad2eb                                                660.1s
 => => resolve nvcr.io/nvidia/pytorch:24.02-py3@sha256:69c54ea51853c57b1f5abae7878a64b238fb10c177855e1c6521d7ab87fad2eb                                                               0.0s
 => [llamafactory internal] load build context                                                                                                                                        0.2s
 => => transferring context: 6.37MB                                                                                                                                                   0.1s
 => [llamafactory 2/7] WORKDIR /app                                                                                                                                                   3.9s
 => [llamafactory 3/7] COPY requirements.txt /app                                                                                                                                     0.1s
 => [llamafactory 4/7] RUN pip config set global.index-url "https://pypi.org/simple" &&     pip config set global.extra-index-url "https://pypi.org/simple" &&     python -m pip in  77.4s
 => [llamafactory 5/7] COPY . /app                                                                                                                                                    0.3s
 => [llamafactory 6/7] RUN EXTRA_PACKAGES="metrics";     if [ "false" == "true" ]; then         EXTRA_PACKAGES="${EXTRA_PACKAGES},bitsandbytes";     fi;     if [ "false" == "true"  19.4s
 => [llamafactory 7/7] RUN pip uninstall -y transformer-engine flash-attn &&     if [ "false" == "true" ]; then         pip uninstall -y ninja && pip install ninja &&         pip i  2.1s
 => [llamafactory] exporting to image                                                                                                                                                 3.4s
 => => exporting layers                                                                                                                                                               3.3s
 => => writing image sha256:269a73554cd54034aa4dd4dd034d90be0c6d4f26730814b5444f4ed4a406572b                                                                                          0.0s
 => => naming to docker.io/library/docker-cuda-llamafactory                                                                                                                           0.0s
[+] Running 2/2
 âœ” Network docker-cuda_default  Created                                                                                                                                               0.1s 
 âœ” Container llamafactory       Started                                                                                                                                               1.1s 
(base) root@ubuntu22:~/data/LLaMA-Factory/docker/docker-cuda# 
```


## æŸ¥çœ‹å®¹å™¨ä¿¡æ¯(å¯é€‰):

ç»ˆç«¯è¿è¡Œä¸‹åˆ—æŒ‡ä»¤æŸ¥çœ‹æœ¬åœ°Dockerä¸»æœºä¸Šå¯ç”¨çš„æ‰€æœ‰Dockeré•œåƒçš„åˆ—è¡¨:

```bash
docker images
```

ç»ˆç«¯æ˜¾ç¤º:

```bash
(base) root@ubuntu22:~/data/LLaMA-Factory/docker/docker-cuda# docker images
REPOSITORY                                                              TAG                        IMAGE ID       CREATED          SIZE
docker-cuda-llamafactory                                                latest                     269a73554cd5   19 minutes ago   22.7GB
```

**"SIZE: 22.7GB"** ï¼ŒLLaMA-Factory ä¾èµ–å®‰è£…å®ŒåçœŸå¤§å•Šã€‚è¿™è¿˜æ˜¯æ²¡ä¸‹è½½å¤§æ¨¡å‹å‘¢ã€‚ğŸ« 

ç»ˆç«¯è¿è¡Œä¸‹åˆ—æŒ‡ä»¤æ˜¾ç¤ºå½“å‰è¿è¡Œä¸­çš„Dockerå®¹å™¨çš„åˆ—è¡¨:

```bash
docker ps
```

ç»ˆç«¯æ˜¾ç¤º:

```log
(base) root@ubuntu22:~/data/LLaMA-Factory/docker/docker-cuda# docker ps
CONTAINER ID   IMAGE                                                                          COMMAND                   CREATED             STATUS             PORTS                                                                                                      NAMES
30a7aaa46cca   docker-cuda-llamafactory                                                       "/opt/nvidia/nvidia_â€¦"   19 minutes ago      Up 19 minutes      0.0.0.0:7860->7860/tcp, :::7860->7860/tcp, 6006/tcp, 8888/tcp, 0.0.0.0:8000->8000/tcp, :::8000->8000/tcp   llamafactory
```

- **PORTS:** 
  - `0.0.0.0:7860->7860/tcp` (è¡¨ç¤ºæœ¬åœ°ä¸»æœºçš„7860ç«¯å£æ˜ å°„åˆ°å®¹å™¨çš„7860ç«¯å£)
  - `0.0.0.0:8000->8000/tcp` (è¡¨ç¤ºæœ¬åœ°ä¸»æœºçš„8000ç«¯å£æ˜ å°„åˆ°å®¹å™¨çš„8000ç«¯å£)
  - å¦å¤–è¿˜æœ‰6006/tcpå’Œ8888/tcpï¼ˆæœªæ˜ å°„åˆ°å¤–éƒ¨ä¸»æœºï¼‰ã€‚


## è¿›å…¥llamafactoryå®¹å™¨æ“ä½œ:

ç»ˆç«¯è¿è¡Œä¸‹åˆ—æŒ‡ä»¤ï¼Œåœ¨å·²ç»è¿è¡Œçš„åä¸º llamafactory çš„å®¹å™¨ä¸­å¯åŠ¨ä¸€ä¸ªæ–°çš„ Bash shell:

```bash
docker compose exec llamafactory bash
```

> æ³¨æ„: ä¸Šè¿°æŒ‡ä»¤éœ€è¦åœ¨ `docker-compose.yml` æ‰€åœ¨ç›®å½•è¿è¡Œã€‚

ç»ˆç«¯ç¤ºä¾‹:

```bash
(base) root@ubuntu22:~/data/LLaMA-Factory/docker/docker-cuda# docker compose exec llamafactory bash
root@30a7aaa46cca:/app# pwd
/app
root@30a7aaa46cca:/app# ls -l
total 148
-rw-r--r--  1 root root  1378 Aug 21 05:49 CITATION.cff
-rw-r--r--  1 root root 11324 Aug 21 05:49 LICENSE
-rw-r--r--  1 root root    33 Aug 21 05:49 MANIFEST.in
-rw-r--r--  1 root root   242 Aug 21 05:49 Makefile
-rw-r--r--  1 root root 49429 Aug 21 05:49 README.md
-rw-r--r--  1 root root 49184 Aug 21 05:49 README_zh.md
drwxr-xr-x  2 root root    83 Aug 21 05:49 assets
drwxr-xr-x  6 root root  4096 Aug 21 05:49 data
drwxr-xr-x  5 root root    44 Aug 21 05:49 evaluation
drwxr-xr-x 10 root root   182 Aug 21 05:49 examples
drwxr-xr-x  2 root root     6 Aug 21 06:19 output
-rw-r--r--  1 root root   645 Aug 21 05:49 pyproject.toml
-rw-r--r--  1 root root   289 Aug 21 05:49 requirements.txt
drwxr-xr-x  2 root root  4096 Aug 21 05:49 scripts
-rw-r--r--  1 root root  3393 Aug 21 05:49 setup.py
drwxr-xr-x  1 root root    35 Aug 21 06:18 src
drwxr-xr-x  5 root root    43 Aug 21 05:49 tests
root@30a7aaa46cca:/app#
```

å‘ç°äº†å—ï¼Ÿå®¹å™¨å†…éƒ¨çš„æ„æˆå’Œ `LLaMA-Factory` é¡¹ç›®ä¸­çš„ä»£ç æ˜¯ä¸€æ ·çš„ï¼Œè€Œä¸”ç°åœ¨ä½ å·²ç»å®‰è£…å¥½äº†æ‰€æœ‰ä¾èµ–ï¼Œæ¥ä¸‹æ¥å’Œå…¶ä»–æ•™ç¨‹çš„æ“ä½œæ–¹å¼éƒ½æ˜¯ä¸€è‡´çš„ã€‚ğŸŒˆ


## é…ç½®ä½¿ç”¨é­”æ­ç¤¾åŒºä¸‹è½½æ¨¡å‹(å¯é€‰):

> ç¬”è€…çš„æœåŠ¡å™¨è¿æ¥ä¸åˆ°hugging_faceï¼Œæ‰€ä»¥ä½¿ç”¨çš„é­”æ­ã€‚

```bash
# ç»ˆç«¯ä¸´æ—¶å¯ç”¨ä» ModelScope Hub è·å–æ¨¡å‹
export USE_MODELSCOPE_HUB=1
```

`LLaMA-Factory 0.8.4.dev0` ç‰ˆæœ¬çš„ä¾èµ–å°‘äº† `modelscope`ï¼Œéœ€è¦è‡ªå·±å®‰è£…ä¸‹ï¼Œå¦åˆ™æ— æ³•ä» ModelScope Hub è·å–æ¨¡å‹:

```bash
# å®‰è£…æˆ–å‡çº§modelscope
pip install modelscope -U
```

## å¯åŠ¨ LLaMA Board å¯è§†åŒ–å¾®è°ƒï¼ˆç”± Gradio é©±åŠ¨ï¼‰:

ä»¥LLaMA Boardä¸ºä¾‹æ¼”ç¤ºï¼Œè¿›ä¸€æ­¥è¯´æ˜Dockeréƒ¨ç½²å’Œå…¶ä»–æ–¹å¼å¹¶æ²¡æœ‰ä¸åŒã€‚ğŸ–ï¸

ç»ˆç«¯è¿è¡Œä»¥ä¸‹æŒ‡ä»¤å¼€å¯å¯è§†åŒ–ç•Œé¢:

```bash
llamafactory-cli webui
```

ç»ˆç«¯ç¤ºä¾‹:

```log
root@30a7aaa46cca:/app# llamafactory-cli webui
Running on local URL:  http://0.0.0.0:7860

To create a public link, set `share=True` in `launch()`.
```

æ­¤æ—¶æœ¬åœ°æµè§ˆå™¨è¾“å…¥ `http://localhost:7860/` å³å¯æ‰“å¼€ç•Œé¢ï¼Œæ•ˆæœå¦‚ä¸‹:

![](../docs/docker_cuda_UI.png)

é€‰æ‹© **"Chat"** ï¼Œç„¶åé€‰æ‹©æ¨¡å‹ï¼Œç‚¹å‡» "åŠ è½½æ¨¡å‹" æµ‹è¯•ä¸‹æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸æ‹‰å–:

![](../docs/é­”æ­æ‹‰å–æ¨¡å‹.png)

ä»¥ä¸‹æ˜¯ç¬”è€…æˆªå–çš„ç»ˆç«¯æ‹‰å–æ¨¡å‹çš„ä¿¡æ¯ï¼Œå¯ä»¥çœ‹å‡ºä¸€åˆ‡ä½¿ç”¨éƒ½æ˜¯æ­£å¸¸çš„ã€‚ğŸŒˆ

```log
root@30a7aaa46cca:/app# llamafactory-cli webui
Running on local URL:  http://0.0.0.0:7860

To create a public link, set `share=True` in `launch()`.
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.15k/6.15k [00:00<00:00, 21.1MB/s]
tokenization_chatglm.py: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.9k/15.9k [00:00<00:00, 58.5MB/s]
tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.62M/2.62M [00:00<00:00, 20.1MB/s]
[INFO|tokenization_utils_base.py:2289] 2024-08-21 07:00:14,511 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--THUDM--glm-4-9b-chat/snapshots/f6e0743b285dd808084530f070ad08e504386750/tokenizer.model
[INFO|tokenization_utils_base.py:2289] 2024-08-21 07:00:14,511 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2289] 2024-08-21 07:00:14,511 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2289] 2024-08-21 07:00:14,511 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--glm-4-9b-chat/snapshots/f6e0743b285dd808084530f070ad08e504386750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2289] 2024-08-21 07:00:14,511 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2533] 2024-08-21 07:00:15,511 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
08/21/2024 07:00:15 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.44k/1.44k [00:00<00:00, 9.54MB/s]
[INFO|configuration_utils.py:733] 2024-08-21 07:00:16,089 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--glm-4-9b-chat/snapshots/f6e0743b285dd808084530f070ad08e504386750/config.json
configuration_chatglm.py: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.27k/2.27k [00:00<00:00, 9.28MB/s]
[INFO|configuration_utils.py:733] 2024-08-21 07:00:17,011 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--glm-4-9b-chat/snapshots/f6e0743b285dd808084530f070ad08e504386750/config.json
[INFO|configuration_utils.py:800] 2024-08-21 07:00:17,012 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/glm-4-9b-chat--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/glm-4-9b-chat--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/glm-4-9b-chat--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/glm-4-9b-chat--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "THUDM/glm-4-9b-chat--modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": true,
  "vocab_size": 151552
}

08/21/2024 07:00:17 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
modeling_chatglm.py: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.3k/47.3k [00:00<00:00, 73.2MB/s]
model.safetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.1k/29.1k [00:00<00:00, 49.7MB/s]
[INFO|modeling_utils.py:3644] 2024-08-21 07:00:18,803 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--THUDM--glm-4-9b-chat/snapshots/f6e0743b285dd808084530f070ad08e504386750/model.safetensors.index.json
Downloading shards:   0%|                                                                                                                                           | 0/10 [00:00<?, ?it/s]
model-00001-of-00010.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 1.79G/1.95G [03:27<04:07, 614kB/s]
```


## æ•°æ®å·è¯¦æƒ…(å¯é€‰):

ä½œä¸ºè¡¥å……å†…å®¹ï¼Œè¿™é‡Œä»‹ç»ä¸‹Dockerè¿è¡Œllamafactoryåå®¹å™¨åï¼Œæ•°æ®çš„å­˜å‚¨ä½ç½®:

- `hf_cache`ï¼šä½¿ç”¨å®¿ä¸»æœºçš„ Hugging Face ç¼“å­˜æ–‡ä»¶å¤¹ï¼Œå…è®¸æ›´æ”¹ä¸ºæ–°çš„ç›®å½•ã€‚

- `ms_cache`ï¼šç±»ä¼¼ Hugging Face ç¼“å­˜æ–‡ä»¶å¤¹ï¼Œä¸º ModelScope ç”¨æˆ·æä¾›ã€‚

- `data`ï¼šå®¿ä¸»æœºä¸­å­˜æ”¾æ•°æ®é›†çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚

- `output`ï¼šå°†å¯¼å‡ºç›®å½•è®¾ç½®ä¸ºè¯¥è·¯å¾„åï¼Œå³å¯åœ¨å®¿ä¸»æœºä¸­è®¿é—®å¯¼å‡ºåçš„æ¨¡å‹ã€‚

å¦‚æœä½ ä¸äº†è§£æ•°æ®å·çš„æ¦‚å¿µï¼Œå»ºè®®æŸ¥çœ‹ç¬”è€…çš„[Docerfile è¯­æ³•æ•™ç¨‹](https://github.com/peilongchencc/docker_tutorial/tree/main/dockerfile_command#docker-composeyml%E4%B8%AD%E7%9A%84volumes%E5%8F%AF%E9%80%89)ã€‚


## æ— æ³•ä»¥Dockeræ–¹å¼å¯åŠ¨llamafactoryè§£å†³æ–¹æ¡ˆ(å¯é€‰):

å¦‚æœä½ åœ¨ä»¥Dockeræ–¹å¼éƒ¨ç½²llamafactoryåï¼Œå‘ç°æ— æ³•å¯åŠ¨llamafactoryå®¹å™¨ï¼Œä¸”é”™è¯¯å¦‚ä¸‹:

> ç¬”è€…ä½¿ç”¨çš„ ubuntu 22.04

```log
(base) root@ubuntu22:~/data/LLaMA-Factory-main# cd docker/docker-cuda/
(base) root@ubuntu22:~/data/LLaMA-Factory-main/docker/docker-cuda# docker compose up -d
[+] Running 0/1
 â ¼ Container llamafactory  Starting                                                                                                                                                                                    0.5s 
Error response from daemon: could not select device driver "nvidia" with capabilities: [[gpu]]
(base) root@ubuntu22:~/data/LLaMA-Factory-main/docker/docker-cuda# docker ps
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
(base) root@ubuntu22:~/data/LLaMA-Factory-main/docker/docker-cuda# docker images
```

è¿™æ˜¯å› ä¸ºä½ çš„æœåŠ¡å™¨æ²¡æœ‰é…ç½® **"æ˜¾å¡ç›´é€š"** ï¼ŒDockerå®¹å™¨æ— æ³•ä½¿ç”¨GPUï¼Œéœ€è¦å®‰è£… **"nvidia-container-toolkit"**ã€‚

å¯ä»¥ä»ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ä¸­ä»»é€‰å…¶ä¸€ï¼Œä¸‹è½½ã€å®‰è£… **"nvidia-container-toolkit"**:

### æ–¹æ³•ä¸€: nvidiaå®˜æ–¹æä¾›çš„å®‰è£…æ–¹å¼:

å¦‚æœä½ çš„æœåŠ¡å™¨èƒ½å¤Ÿæ­£å¸¸ä»nvidiaå®˜æ–¹è·å–èµ„æºï¼Œé‚£å°±éå¸¸æ–¹ä¾¿äº†ã€‚æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œå³å¯:

> è¯¦ç»†ä¿¡æ¯å¯è®¿é—® "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-the-nvidia-container-toolkit" æŸ¥çœ‹ã€‚

#### 1. é…ç½®ä»£ç ä»“åº“:

```bash
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

#### 2. æ›´æ–°åŒ…åˆ—è¡¨:

```bash
sudo apt update
```

#### 3. å®‰è£…NVIDIA Container Toolkit:

```bash
sudo apt-get install -y nvidia-container-toolkit
```

è¿™å°±æˆåŠŸäº†ï½

### æ–¹æ³•äºŒ: æ‰‹åŠ¨å®‰è£…NVIDIA Container Toolkit:

å¦‚æœä½ å’Œç¬”è€…ä¸€æ ·æœåŠ¡å™¨ç½‘ç»œå—é™ï¼Œæ— æ³•æ‹‰å–ç±»ä¼¼ "https://nvidia.github.io" çš„ç½‘å€ã€‚é‚£å°±éœ€è¦æ‰‹åŠ¨ä¸‹è½½ **NVIDIA Container Toolkit** è¿›è¡Œå®‰è£…äº†ã€‚æ­¥éª¤å¦‚ä¸‹:

#### 1. ä¸‹è½½å®‰è£…åŒ…:

è¯·æ ¹æ®ä»¥ä¸‹ç½‘å€è®¿é—®NVIDIA Container Toolkitçš„GitHubï¼Œé€‰æ‹©è‡ªå·±ç³»ç»Ÿå¯¹åº”çš„å®‰è£…åŒ…:

```log
https://github.com/NVIDIA/nvidia-container-toolkit/releases
```

![](../docs/nvidia-container-toolkit.png)


#### 2. å°†å®‰è£…åŒ…ä¸Šä¼ åˆ°æœåŠ¡å™¨:

æ ¹æ®ä¸ªäººä¹ æƒ¯ï¼Œä½¿ç”¨ `scp` æˆ– vscode ç•Œé¢æ‹–æ‹‰ä¸Šä¼ æ–‡ä»¶åˆ° **ä»»æ„ä½ç½®**ã€‚

#### 3. è§£å‹æ–‡ä»¶:

å¦‚æœä½ å’Œç¬”è€…ä½¿ç”¨çš„ç›¸åŒçš„å®‰è£…åŒ…ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹åˆ—æŒ‡ä»¤:

```bash
tar -xvf nvidia-container-toolkit_1.16.1_deb_amd64.tar.gz
```

è§£å‹åï¼Œæ•ˆæœå¦‚ä¸‹:

```log
(base) root@ubuntu22:~/data# tar -xvf nvidia-container-toolkit_1.16.1_deb_amd64.tar.gz
release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container-dev_1.16.1-1_amd64.deb
release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container-tools_1.16.1-1_amd64.deb
release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container1-dbg_1.16.1-1_amd64.deb
release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container1_1.16.1-1_amd64.deb
release-v1.16.1-stable/packages/ubuntu18.04/amd64/nvidia-container-toolkit-base_1.16.1-1_amd64.deb
release-v1.16.1-stable/packages/ubuntu18.04/amd64/nvidia-container-toolkit-operator-extensions_1.16.1-1_amd64.deb
release-v1.16.1-stable/packages/ubuntu18.04/amd64/nvidia-container-toolkit_1.16.1-1_amd64.deb
(base) root@ubuntu22:~/data# 
```

#### 4. æŒ‰ä¾èµ–é¡ºåºå®‰è£… .deb æ–‡ä»¶:

```bash
sudo dpkg -i release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container1_1.16.1-1_amd64.deb \
release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container-tools_1.16.1-1_amd64.deb \
release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container-dev_1.16.1-1_amd64.deb \
release-v1.16.1-stable/packages/ubuntu18.04/amd64/nvidia-container-toolkit-base_1.16.1-1_amd64.deb \
release-v1.16.1-stable/packages/ubuntu18.04/amd64/nvidia-container-toolkit_1.16.1-1_amd64.deb
```

ç»ˆç«¯æ˜¾ç¤º:

```log
(base) root@ubuntu22:~/data# sudo dpkg -i release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container1_1.16.1-1_amd64.deb \
release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container-tools_1.16.1-1_amd64.deb \
release-v1.16.1-stable/packages/ubuntu18.04/amd64/libnvidia-container-dev_1.16.1-1_amd64.deb \
release-v1.16.1-stable/packages/ubuntu18.04/amd64/nvidia-container-toolkit-base_1.16.1-1_amd64.deb \
release-v1.16.1-stable/packages/ubuntu18.04/amd64/nvidia-container-toolkit_1.16.1-1_amd64.deb
æ­£åœ¨é€‰ä¸­æœªé€‰æ‹©çš„è½¯ä»¶åŒ… libnvidia-container1:amd64ã€‚
(æ­£åœ¨è¯»å–æ•°æ®åº“ ... ç³»ç»Ÿå½“å‰å…±å®‰è£…æœ‰ 218002 ä¸ªæ–‡ä»¶å’Œç›®å½•ã€‚)
å‡†å¤‡è§£å‹ .../libnvidia-container1_1.16.1-1_amd64.deb  ...
æ­£åœ¨è§£å‹ libnvidia-container1:amd64 (1.16.1-1) ...
æ­£åœ¨é€‰ä¸­æœªé€‰æ‹©çš„è½¯ä»¶åŒ… libnvidia-container-toolsã€‚
å‡†å¤‡è§£å‹ .../libnvidia-container-tools_1.16.1-1_amd64.deb  ...
æ­£åœ¨è§£å‹ libnvidia-container-tools (1.16.1-1) ...
æ­£åœ¨é€‰ä¸­æœªé€‰æ‹©çš„è½¯ä»¶åŒ… libnvidia-container-dev:amd64ã€‚
å‡†å¤‡è§£å‹ .../libnvidia-container-dev_1.16.1-1_amd64.deb  ...
æ­£åœ¨è§£å‹ libnvidia-container-dev:amd64 (1.16.1-1) ...
æ­£åœ¨é€‰ä¸­æœªé€‰æ‹©çš„è½¯ä»¶åŒ… nvidia-container-toolkit-baseã€‚
å‡†å¤‡è§£å‹ .../nvidia-container-toolkit-base_1.16.1-1_amd64.deb  ...
æ­£åœ¨è§£å‹ nvidia-container-toolkit-base (1.16.1-1) ...
æ­£åœ¨é€‰ä¸­æœªé€‰æ‹©çš„è½¯ä»¶åŒ… nvidia-container-toolkitã€‚
å‡†å¤‡è§£å‹ .../nvidia-container-toolkit_1.16.1-1_amd64.deb  ...
æ­£åœ¨è§£å‹ nvidia-container-toolkit (1.16.1-1) ...
æ­£åœ¨è®¾ç½® libnvidia-container1:amd64 (1.16.1-1) ...
æ­£åœ¨è®¾ç½® libnvidia-container-tools (1.16.1-1) ...
æ­£åœ¨è®¾ç½® libnvidia-container-dev:amd64 (1.16.1-1) ...
æ­£åœ¨è®¾ç½® nvidia-container-toolkit-base (1.16.1-1) ...
æ­£åœ¨è®¾ç½® nvidia-container-toolkit (1.16.1-1) ...
æ­£åœ¨å¤„ç†ç”¨äº libc-bin (2.35-0ubuntu3.1) çš„è§¦å‘å™¨ ...
(base) root@ubuntu22:~/data#
```

#### 5. éªŒè¯å®‰è£…:

å®‰è£…å®Œæˆåï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤éªŒè¯ NVIDIA Container Toolkit æ˜¯å¦æ­£ç¡®å®‰è£…ï¼š

```bash
nvidia-container-cli --version
```

ç»ˆç«¯æ˜¾ç¤º:

```log
(base) root@ubuntu22:~/data# nvidia-container-cli --version
cli-version: 1.16.1
lib-version: 1.16.1
build date: 2024-07-23T14:57+00:00
build revision: 4c2494f16573b585788a42e9c7bee76ecd48c73d
build compiler: x86_64-linux-gnu-gcc-7 7.5.0
build platform: x86_64
build flags: -D_GNU_SOURCE -D_FORTIFY_SOURCE=2 -DNDEBUG -std=gnu11 -O2 -g -fdata-sections -ffunction-sections -fplan9-extensions -fstack-protector -fno-strict-aliasing -fvisibility=hidden -Wall -Wextra -Wcast-align -Wpointer-arith -Wmissing-prototypes -Wnonnull -Wwrite-strings -Wlogical-op -Wformat=2 -Wmissing-format-attribute -Winit-self -Wshadow -Wstrict-prototypes -Wunreachable-code -Wconversion -Wsign-conversion -Wno-unknown-warning-option -Wno-format-extra-args -Wno-gnu-alignof-expression -Wl,-zrelro -Wl,-znow -Wl,-zdefs -Wl,--gc-sections
(base) root@ubuntu22:~/data# 
```

ç°åœ¨ï¼Œä½ å·²ç»æˆåŠŸå®‰è£…äº† NVIDIA Container Toolkit 1.16.1ï¼Œå¹¶æ­£ç¡®é…ç½®äº†ç¯å¢ƒã€‚æ ¹æ® `nvidia-container-cli --version` çš„è¾“å‡ºï¼Œå·¥å…·åŒ…å·²ç»æ­£ç¡®å®‰è£…å¹¶å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚